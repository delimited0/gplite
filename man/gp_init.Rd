% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/gp.R
\name{gp_init}
\alias{gp_init}
\title{Initialize a GP model}
\usage{
gp_init(
  cfs = cf_sexp(),
  lik = lik_gaussian(),
  method = "full",
  num_basis = 100,
  num_inducing = 100,
  seed = 12345
)
}
\arguments{
\item{cfs}{The covariance function(s). Either a single covariance function or a list of them. See \code{\link{cf}}.}

\item{lik}{Likelihood (observation model). See \code{\link{lik}}.}

\item{method}{Method for approximating the covariance function, can be one of 
\code{'full'} (full exact GP),
\code{'rf'} (random features), or 
\code{'fitc'} (fully independent training conditional, FITC). See below for details.}

\item{num_basis}{Number of basis functions in the covariance approximation for 'rf' and other
basis function methods.}

\item{num_inducing}{Number of inducing points for FITC approximation.}

\item{seed}{Seed for reproducible results.}
}
\value{
A GP model object that can be passed to other functions, for example when optimizing the hyperparameters or making predictions.
}
\description{
Initializes a GP model with given covariance function(s) and likelihood. The model can then be fitted using \code{\link{gp_fit}} or \code{\link{gp_mcmc}}. For hyperparameter optimization, see \code{\link{gp_optim}}
}
\details{
The argument \code{method} defines the method for approximating the covariance
function calculation. The choices are:
\itemize{
 \item{\code{'full'}:} {
 Full exact covariance function is used, meaning
that the inference will be for the \code{n} latent
function values (fitting time scales cubicly in \code{n}).
}
 \item{\code{'rf'}:} {
 Uses random features (or basis functions) for approximating the covariance function,
 which means the inference
time scales cubicly in the number of approximating basis functions \code{num_basis}.
For stationary covariance functions random Fourier features (Rahimi and Recht, 2007)
is used, and for non-stationary kernels using case specific method when possible 
(for example, drawing the hidden layer parameters randomly for \code{cf_nn}). For
\code{cf_const} and \code{cf_lin} this means using standard linear model, and the
inference is performed on the weight space (not in the function space). Thus if
the model is linear (only \code{cf_const} and \code{cf_lin} are used), this will give
a potentially huge speed-up if the number of features is considerably smaller than
the number of data points.
}
 \item{\code{'fitc'}:} {
 Uses the fully independent training conditional, FITC, approximation 
 (see Quiñonero-Candela and Rasmussen, 2005; Snelson and Ghahramani, 2006). 
 The fitting time scales \code{O(n*m^2)}, where n is the number of data points and 
 m the number of inducing points \code{num_inducing}.
 The inducing point locations are chosen using the k-means algorithm.
 }
 }
}
\section{References}{


Rasmussen, C. E. and Williams, C. K. I. (2006). Gaussian processes for machine learning. 
MIT Press.

Rahimi, A. and Recht, B. (2008). Random features for large-scale kernel machines. 
In Advances in Neural Information Processing Systems 20.

Quiñonero-Candela, J. and Rasmussen, C. E (2005). A unifying view of sparse approximate 
Gaussian process regression. Journal of Machine Learning Research 6:1939-1959.

Snelson, E. and Ghahramani, Z. (2006). Sparse Gaussian processes using pseudo-inputs. 
In Advances in Neural Information Processing Systems 18.
}

\examples{
\donttest{

# Generate some toy data
set.seed(1242)
n <- 500
x <- matrix(rnorm(n*3), nrow=n)
f <- sin(x[,1]) + 0.5*x[,2]^2 + x[,3]
y <- f + 0.5*rnorm(n)
x <- data.frame(x1=x[,1], x2=x[,2], x3=x[,3])

# Full GP
gp <- gp_init(cf_sexp())
gp <- gp_optim(gp, x, y)

# Approximate solution using random features
gp <- gp_init(cf_sexp(), method='rf', num_basis=300)
gp <- gp_optim(gp, x, y)

# Approximate solution using FITC
gp <- gp_init(cf_sexp(), method='fitc', num_inducing=100)
gp <- gp_optim(gp, x, y)

}

}
