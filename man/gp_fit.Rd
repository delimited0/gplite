% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/gp.R
\name{gp_fit}
\alias{gp_fit}
\alias{gp_sample}
\title{Fit a GP model}
\usage{
gp_fit(gp, x, y, trials = NULL, jitter = 0.001, ...)

gp_sample(gp, x, y, trials = NULL, jitter = 0.001, ...)
}
\arguments{
\item{gp}{The gp model object to be fitted.}

\item{x}{n-by-d matrix of input values (n is the number of observations and d the input dimension). 
Can also be a vector of length n if the model has only a single input.}

\item{y}{Vector of n output (target) values.}

\item{trials}{Vecton of length n giving the number of trials for each observation in binomial 
(and beta binomial) model.}

\item{jitter}{Magnitude of diagonal jitter for covariance matrices for numerical stability.}

\item{...}{Further arguments to be passed to \link{rstan}'s function \link{optimizing} (if \code{gp_fit} or
\code{gp_optim} was called) or \link{sampling} (when \code{gp_sample} was called).}
}
\value{
An updated GP model object.
}
\description{
Function \code{gp_fit} fits a GP model with the current hyperparameters. Notice that this function 
does not optimize the hyperparameters in any way, but only finds the Laplace approximation (or the analytical 
true posterior in the case of Gaussian likelihood) to the latent values. Function \code{gp_sample} 
draws from the posterior of the latent values given the current hyperparameter estimates.
}
\section{References}{


Rasmussen, C. E. and Williams, C. K. I. (2006). Gaussian processes for machine learning. MIT Press.
}

\examples{
\donttest{
# Analytic approximation
cf <- gpcf_sexp()
lik <- lik_binomial()
gp <- gp_init(cf, lik)
gp <- gp_fit(gp, x, y)

# MCMC solution
gp <- gp_sample(gp, x, y, trials=trials, chains=2, iter=1000)
}

}
